# 1. 副本机制

在前面章节中，我们创建的Pod都是未经托管的，而是选择集群中的一个节点来运行Pod，然后在节点上运行容器，但是这样的话，当节点崩溃或者是网络断开，Pod不会自动被重新创建，对应的就是Pod丢失，另一个问题就是如果直接管理散乱分布在集群中的Pod十分复杂，其中包括如何保证集群内可用Pod的数量，如何为所有Pod更新镜像，更新镜像过程中，如何保证服务的可用性，如果实现快速回滚等等。

为了解决上面提到的问题，在K8s中，通常不会直接创建Pod，而是针对不同的场景通过创建不同的副本控制器来部署和发布Pod，本章节就对常见的副本控制器进行阐述。


# 2. ReplicationController

ReplicationController（简称rc）是一种K8s资源，可确保它的Pod始终保持运行状态。如果Pod因任何原因消失（如节点从集群中消失或由于该Pod已从节点中逐出），则rc会注意到缺少了Pod并创建替代Pod。

rc（以及下面所有的控制器）都是通过标签选择器来选择哪些Pod是归自己的管的，反过来，Pod的OwnerReference元数据会指向创建它的控制器。rc（以及下面所有的控制器）通过比较Spec中指定的Pod数量与实际运行的Pod数量，来决策是否创建Pod、删除Pod或者维持不变（控制循环的实现）。

## 2.1 创建ReplicationController

kubia-rc.yaml文件如下

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia:v1
        ports:
        - containerPort: 8080
```
文件解析

* apiVersion用来指明该描述文件遵循v1版本的K8s Api
* kind用来指明本段代码描述的是一个ReplicationController类型的资源
* metadata为该Pod的元数据，其中设置了该ReplicationController的name为kubia
* spec用来阐述该ReplicationController的期望状态，里面有三个主要的部分，一个是replicas（副本个数）用来指定应运行的Pod数量，一个是selector（标签选择器），用来表明该ReplicationController根据app: kubia这一个标签来确定该Controller作用域中有哪些pod，一个是template（Pod模板），用于创建新的Pod副本，在本例中，该容器使用的镜像为luksa/kubia:v1，名字是kubia，同时暴露出的端口是tcp:8080。

*注：不指定标签选择器也是一种选择，在这种情况下，Controller会自动根据Pod模板中的标签自动配置*

接下来运行kubectl命令

```
sudo kubectl create -f kubia-rc.yaml
```

查看ReplicationController的创建情况
```
master@k8s-master:~$ sudo kubectl get rc
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       26s
```

查看ReplicationController的详细信息
```
master@k8s-master:~$ sudo kubectl describe rc kubia
Name:         kubia
Namespace:    default
Selector:     app=kubia
Labels:       app=kubia
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        luksa/kubia:v1
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  62s   replication-controller  Created pod: kubia-lsslw
  Normal  SuccessfulCreate  62s   replication-controller  Created pod: kubia-s2r6h
  Normal  SuccessfulCreate  62s   replication-controller  Created pod: kubia-pswhv
```

## 2.2 使用ReplicationController

首先查看一下通过ReplicationController创建的Pod情况
```
master@k8s-master:~$ sudo kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
kubia-lsslw   1/1     Running   0          13m
kubia-pswhv   1/1     Running   0          13m
kubia-s2r6h   1/1     Running   0          13m
```
我们查看利用如下命令来查看任一个pod的信息
```
sudo kubectl get pod kubia-lsslw -o yaml
```
可以看到Pod的ownerReferences元数据显示如下：
```
ownerReferences:
  - apiVersion: v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicationController
    name: kubia
    uid: 3f2b9712-3c70-4e2a-85ff-fcbe798a56a5
```
可见，该Pod确实是由kubia这个ReplicationController创建的。

下面尝试将Pod移入移除ReplicationController作用域。ReplicationController通过标签选择器来匹配Pod，因此可以通过修改Pod的标签来将其移入或者移除ReplicationController的作用域。

首先，我们修改其中一个Pod的app标签，然后再来查看一些会发生什么事情。

```
master@k8s-master:~$ sudo kubectl label pod kubia-lsslw app=nginx --overwrite
pod/kubia-lsslw labeled
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-lr57g   1/1     Running   0          6s    app=kubia
kubia-lsslw   1/1     Running   0          33m   app=nginx
kubia-pswhv   1/1     Running   0          33m   app=kubia
kubia-s2r6h   1/1     Running   0          33m   app=kubia
```
因为我们修改了kubia-lsslw这个Pod的标签（app=nginx），导致受ReplicationController管控的Pod数量减少到2，小于ReplicationController中指定的replicas（3），因此ReplicationController会重新创建一个Pod（kubia-lr57g）

再次测试，我们删除受ReplicationController管控的任一Pod，如kubia-s2r6h，来看看发生什么事情

```
master@k8s-master:~$ sudo kubectl delete pod kubia-s2r6h
pod "kubia-s2r6h" deleted
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE     LABELS
kubia-lr57g   1/1     Running   0          6m12s   app=kubia
kubia-lsslw   1/1     Running   0          39m     app=nginx
kubia-pswhv   1/1     Running   0          39m     app=kubia
kubia-sljhj   1/1     Running   0          94s     app=kubia
```

可以看到虽然Pod kubia-s2r6h被删除，但是ReplicationController又重新拉起了一个Pod kubia-sljhj，来保证当前集群中对应的可用Pod数量为3。

接下来我们再尝试将Pod kubia-lsslw的app标签改回来，这时候讲道理集群中会出现4个具有相同标签（app=kubia）的Pod，我们来看看ReplicationController会怎么做

 ```
master@k8s-master:~$ sudo kubectl label pod kubia-lsslw app=kubia --overwrite
pod/kubia-lsslw labeled
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS        RESTARTS   AGE     LABELS
kubia-lr57g   1/1     Running       0          10m     app=kubia
kubia-lsslw   1/1     Running       0          44m     app=kubia
kubia-pswhv   1/1     Running       0          44m     app=kubia
kubia-sljhj   1/1     Terminating   0          5m56s   app=kubia
 ```
 
可以看到ReplicationController终结了一个Pod kubia-sljhj，来保证当前集群中对应的可用Pod数量为3。
 
综合上面的实践，我们可以看到使用ReplicationController（其实还有其它控制器）来创建Pod，每时每刻都能够保证集群中可用Pod的数量。
 

## 2.3 水平缩放ReplicationController

使用kubectl scale rc rc_name --replicas=n命令实现ReplicationController的缩放

比如，我们将ReplicationController kubia的副本数量由3扩展到4

```
master@k8s-master:~$ sudo kubectl scale rc kubia --replicas=4
[sudo] password for master: 
replicationcontroller/kubia scaled
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-4kzwj   1/1     Running   0          5s    app=kubia
kubia-lr57g   1/1     Running   0          31m   app=kubia
kubia-lsslw   1/1     Running   0          64m   app=kubia
kubia-pswhv   1/1     Running   0          64m   app=kubia
```

然后再将ReplicationController kubia的副本数量由4扩展到3

```
master@k8s-master:~$ sudo kubectl scale rc kubia --replicas=3
replicationcontroller/kubia scaled
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS        RESTARTS   AGE   LABELS
kubia-4kzwj   1/1     Terminating   0          48s   app=kubia
kubia-lr57g   1/1     Running       0          31m   app=kubia
kubia-lsslw   1/1     Running       0          65m   app=kubia
kubia-pswhv   1/1     Running       0          65m   app=kubia
```

除了kubectl scale命令，还可以通过编辑定义来缩放ReplicationController，即通过命令
```
sudo kubectl edit rc kubia
```
打开ReplicationController kubia的定义文件，找到spec.replicas字段，将其更改为4，保存退出，接着查询生效

```
master@k8s-master:~$ sudo kubectl edit rc kubia
replicationcontroller/kubia edited
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-j8vj7   1/1     Running   0          5s    app=kubia
kubia-lr57g   1/1     Running   0          35m   app=kubia
kubia-lsslw   1/1     Running   0          69m   app=kubia
kubia-pswhv   1/1     Running   0          69m   app=kubia
```

## 2.4 移除ReplicationController

我们可以通过kubectl delete来删除ReplicationController，并且通过--cascade选项来决定，删除ReplicationController是否删除其管理的Pod

删除ReplicationController的同时也删除Pod
```
sudo kubectl delete rc kubia
```

只删除ReplicationController，保留Pod，但是此时Pod已经独立，不受任何控制器的管理。我们可以使用适当的标签选择器创建新的控制器将这些独立的Pod再管理起来。
```
sudo kubectl delete rc kubia --cascade=false
```

# 3. ReplicaSet

最初，ReplicationController是用于复制和异常时重新调度节点的唯一K8s组件，后面又引入了名为ReplicaSet这一个副本控制器将ReplicationController代替，但是两者的行为完全相同，只是ReplicaSet的标签选择器的表达能力更强。具体的，ReplicationController只能选择包含某一个标签匹配的Pod，而ReplicaSet还可以允许多个标签的同时匹配，还支持集合型选择器。换句话说ReplicaSet是表达能力更强的增强型ReplicationController，在其它方面，两种没有什么不同。

## 3.1 创建ReplicaSet

kubia-replicaset.yaml文件如下

```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia:v1
        ports:
        - containerPort: 8080
```
文件解析

* apiVersion用来指明该描述文件遵循apps/v1版本的K8s Api
* kind用来指明本段代码描述的是一个ReplicaSet类型的资源
* metadata为该Pod的元数据，其中设置了该ReplicaSet的name为kubia
* spec用来阐述该ReplicaSet的期望状态，里面有三个主要的部分，一个是replicas（副本个数）用来指定应运行的Pod数量，一个是selector（标签选择器），用来表明该ReplicaSet根据app: kubia这一个标签来确定该Controller作用域中有哪些pod，一个是template（Pod模板），用于创建新的Pod副本，在本例中，该容器使用的镜像为luksa/kubia:v1，名字是kubia，同时暴露出的端口是tcp:8080。


接下来运行kubectl命令

```
sudo kubectl create -f kubia-replicaset.yaml
```

查看ReplicaSet的创建情况
```
master@k8s-master:~$ sudo kubectl get rs
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       58s
```

查看ReplicaSet的详细信息
```
master@k8s-master:~$ sudo kubectl describe rs kubia
Name:         kubia
Namespace:    default
Selector:     app=kubia
Labels:       <none>
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        luksa/kubia:v1
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  77s   replicaset-controller  Created pod: kubia-stkxj
  Normal  SuccessfulCreate  77s   replicaset-controller  Created pod: kubia-fpz6j
  Normal  SuccessfulCreate  77s   replicaset-controller  Created pod: kubia-v4qkc
```

## 3.2 ReplicaSet中的标签选择器

ReplicaSet中的标签选择器具有比ReplicationController中的标签选择器更强大的表达能力，除了支持相等型选择器，还支持集合型选择器，并且支持多个选择器的同时满足。

### 3.2.1 相等型标签选择器模板

```
selector:
    matchLabels:
      app: kubia
      env: proc
```

matchLabels指定的是相等型选择器，上述选择器选择的是同时包含app=kubia和env=proc两个标签的Pod

### 3.2.2 集合型标签选择器模板

```
selector:
    matchExpressions:
      key: app
      operator: In
      values:
        - kubia
```

matchExpressions指定的是集合型选择器，每一个表达式都必须包含一个key和一个operator（运算符），可能还会包含一个values列表（取决于运算符）。

目前有四种有效的运算符：
* In: Lable的值必须与其中一个指定的values匹配
* NotIn: Lable的值与任意一个指定的values都不匹配
* Exists: Pod必须包含一个指定名称的标签（值不重要），在使用此运算符时，不应指定values字段
* DoesNotExist: Pod不得包含指定名称的标签，在使用此运算符时，不应指定values字段


### 3.2.3 多条件选择器

```
selector:
    matchLabels:
      app: kubia
      env: proc
    matchExpressions:
      key: app
      operator: In
      values:
        - kubia
```

当指定多个选择器时，各个选择器之间是逻辑与的关系，即所有选择器的条件都满足的Pod才会被选择。

### 3.3 其他操作

ReplicaSet与ReplicationController行为完全相同，因此其余操作参看ReplicationController即可，这里不再赘述。

# 4. Deployment

虽然上面描述了ReplicaSet与ReplicationController，但是一般情况下，管理员不会直接创建ReplicaSet与ReplicationController，而是通过创建Deployment这一个控制器来实现对Pod的创建和管理。本质上，创建Deployment后，Deployment控制器将立刻创建一个ReplicaSet副本集，并由ReplicaSet创建所需要的Pod，其目的就是在保留原来优点的前提下，更好地实现版本的更新与回滚。

## 4.1 创建Deployment

kubia-deployment.yaml文件如下

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia:v1
        ports:
        - containerPort: 8080
```
文件解析

* apiVersion用来指明该描述文件遵循apps/v1版本的K8s Api
* kind用来指明本段代码描述的是一个Deployment类型的资源
* metadata为该Pod的元数据，其中设置了该Deployment的name为kubia
* spec用来阐述该Deployment的期望状态，里面有三个主要的部分，一个是replicas（副本个数）用来指定应运行的Pod数量，一个是selector（标签选择器），用来表明该ReplicaSet根据app: kubia这一个标签来确定该Controller作用域中有哪些pod，一个是template（Pod模板），用于创建新的Pod副本，在本例中，该容器使用的镜像为luksa/kubia:v1，名字是kubia，同时暴露出的端口是tcp:8080。

接下来运行kubectl命令

```
sudo kubectl create -f kubia-deployment.yaml
```

查看Deployment的创建情况
```
master@k8s-master:~$ sudo kubectl get deployment
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
kubia   3/3     3            3           13s

```

查看Deployment的详细信息
```
master@k8s-master:~$ sudo kubectl describe deployment kubia
Name:                   kubia
Namespace:              default
CreationTimestamp:      Tue, 22 Dec 2020 21:45:24 -0800
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=kubia
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        luksa/kubia:v1
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kubia-5f78bc5dc5 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  33s   deployment-controller  Scaled up replica set kubia-5f78bc5dc5 to 3
```

可以看到该Deployment创建了一个ReplicaSet，名称为kubia-5f78bc5dc5。

## 4.2 查看Pod状态

Deployment通过创建ReplicaSet进而创建和管理Pod。我们首先查看一下ReplicaSet kubia-5f78bc5dc5的ownerReferences元数据

```
sudo kubectl get rs kubia-5f78bc5dc5 -o yaml
```
可以看到
```
ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubia
      uid: e8786a6a-ffb9-4e4c-bcc5-21eea0f2f8f2
```
说明ReplicaSet kubia-5f78bc5dc5确实是由Deployment kubia创建的，然后我们查看ReplicaSet kubia-5f78bc5dc5创建Pod的情况
```
master@k8s-master:~$ sudo kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
kubia-5f78bc5dc5-lcd2z   1/1     Running   0          8m45s
kubia-5f78bc5dc5-r4cwd   1/1     Running   0          8m45s
kubia-5f78bc5dc5-s65f7   1/1     Running   0          8m45s
```
通过查看任一Pod的ownerReferences元数据，确认这些Pod是由ReplicaSet kubia-5f78bc5dc5创建得到。

综上，证明了Deployment通过创建ReplicaSet进而创建和管理Pod。


## 4.3 更新镜像

我们来模拟更新Deployment镜像的过程，首先在worker节点上使用docker tag命令来创建一个新版本的镜像，如下
```
sudo docker tag luksa/kubia:v1 luksa/kubia:v2
```

首先查看当前的Deployment kubia的镜像，如下所示，采用的是luksa/kubia:v1
```
master@k8s-master:~$ sudo kubectl describe deployment kubia
Name:                   kubia
Namespace:              default
CreationTimestamp:      Tue, 22 Dec 2020 21:45:24 -0800
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=kubia
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        luksa/kubia:v1
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kubia-5f78bc5dc5 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  21m   deployment-controller  Scaled up replica set kubia-5f78bc5dc5 to 3
```

然后执行下面的命令将Deployment的镜像更改为luksa/kubia:v2,

```
sudo kubectl set image deployment.v1.apps/kubia kubia=luksa/kubia:v2
```
其中，deployment.v1.apps/kubia 为是固定写法，deployment是资源名、v1是资源版本、apps是资源组，这里也可以简写为deployment或者deployment.apps，比如说写为deployment的时候，默认将使用apps组 v1版本，后面的kubia为Deployment的名称。

后面的kubia=luksa/kubia:v2，kubia为Pod中容器的名字，因为Pod中可能有多个容器，因此要指定容器的名字，luksa/kubia:v2为镜像名。

```
master@k8s-master:~$ sudo kubectl set image deployment.v1.apps/kubia kubia=luksa/kubia:v2
deployment.apps/kubia image updated
master@k8s-master:~$ sudo kubectl describe deployment kubia
Name:                   kubia
Namespace:              default
CreationTimestamp:      Tue, 22 Dec 2020 21:45:24 -0800
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=kubia
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        luksa/kubia:v2
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kubia-55d48448fd (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  29m   deployment-controller  Scaled up replica set kubia-5f78bc5dc5 to 3
  Normal  ScalingReplicaSet  7s    deployment-controller  Scaled up replica set kubia-55d48448fd to 1
  Normal  ScalingReplicaSet  6s    deployment-controller  Scaled down replica set kubia-5f78bc5dc5 to 2
  Normal  ScalingReplicaSet  6s    deployment-controller  Scaled up replica set kubia-55d48448fd to 2
  Normal  ScalingReplicaSet  4s    deployment-controller  Scaled down replica set kubia-5f78bc5dc5 to 1
  Normal  ScalingReplicaSet  4s    deployment-controller  Scaled up replica set kubia-55d48448fd to 3
  Normal  ScalingReplicaSet  2s    deployment-controller  Scaled down replica set kubia-5f78bc5dc5 to 0
```
可以看到镜像已经更新为luksa/kubia:v2。

## 4.4 滚动升级

在上文中通过更改镜像触发了Deployment的更新，那么Deployment是如何更新的呢。其实，Deployment的每一次更新都会创建新的ReplicaSet，在更新过程中，逐渐删除旧ReplicaSet管理的Pod，同时逐渐创建新ReplicaSet管理的Pod，从而使得在整个更新的过程中应用都处于一个可用状态，这种更新策略叫做滚动升级策略，是Deployment默认的更新策略。通过下面的信息我们可以看到滚动升级的过程。

```
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  29m   deployment-controller  Scaled up replica set kubia-5f78bc5dc5 to 3
  Normal  ScalingReplicaSet  7s    deployment-controller  Scaled up replica set kubia-55d48448fd to 1
  Normal  ScalingReplicaSet  6s    deployment-controller  Scaled down replica set kubia-5f78bc5dc5 to 2
  Normal  ScalingReplicaSet  6s    deployment-controller  Scaled up replica set kubia-55d48448fd to 2
  Normal  ScalingReplicaSet  4s    deployment-controller  Scaled down replica set kubia-5f78bc5dc5 to 1
  Normal  ScalingReplicaSet  4s    deployment-controller  Scaled up replica set kubia-55d48448fd to 3
  Normal  ScalingReplicaSet  2s    deployment-controller  Scaled down replica set kubia-5f78bc5dc5 to 0
```

查看一下ReplicaSet的情况

```
master@k8s-master:~$ sudo kubectl get rs
NAME               DESIRED   CURRENT   READY   AGE
kubia-55d48448fd   3         3         3       11m
kubia-5f78bc5dc5   0         0         0       41m
```
可以看到有两个ReplicaSet，这两个ReplicaSet都是由Deployment创建，并且旧ReplicaSet对应的可用Pod为0，新ReplicaSet对应的可用Pod为3。

在滚动升级的过程中，实际上还有一个**滚动升级策略**，其在yaml中定义如下

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    ...
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    ...
```
主要的就是设定maxSurge和maxUnavailable两个参数。

* maxSurge为滚动更新过程中，可以超出期望副本数的最大值，该取值可以是一个绝对值（如：5），也可以是一个相对于期望副本数的百分比（如：10%）；如果填写百分比，则以期望副本数乘以该百分比后向上取整的方式计算对应的绝对值；例如：假设此值被设定为30%，当滚动更新开始时，新的副本集（ReplicaSet）可以立刻扩容，但是旧Pod和新Pod的总数不超过Deployment期待副本数（spec.repilcas）的130%。一旦旧Pod被终止后，新的副本集可以进一步扩容，但是整个滚动更新过程中，新旧Pod的总数不超过Deployment期待副本数（spec.repilcas）的130%。
* maxUnavailable为滚动更新过程中，不可用副本数的最大值。该取值可以是一个绝对值（例如：5），也可以是一个相对于期望副本数的百分比（例如：10%）；如果填写百分比，则以期望副本数乘以该百分比后向下取整的方式计算对应的绝对值；当最大超出副本数maxSurge为0时，此数值不能为0；默认值为25%；例如：假设此值被设定为30%，当滚动更新开始时，旧的副本集（ReplicaSet）可以缩容到期望副本数的70%；在新副本集扩容的过程中，一旦新的Pod已就绪，旧的副本集可以进一步缩容，整个滚动更新过程中，确保新旧就绪副本数之和不低于期望副本数的70%。是


## 4.5 回滚版本


## 4.6 伸缩Pod


## 4.7 暂停和继续 Deployment


## 4.8 状态查询


## 4.9 Deployment策略





# 5. DaemonSet


# 6. Job


# 7.CronJob


# 8. StatefulSet


# 9. 总结

Pod是Kubernetes中最小的调度单元，我们可以通过kubectl直接创建一个Pod，但是Pod本身并不能自愈（self-healing）。如果一个Pod所在的Node出现故障，Pod将被删除；同理，当因为节点资源不够或节点维护而驱逐Pod时，Pod也将被删除。

Kubernetes通过引入副本控制器的概念来管理Pod实例。在Kubernetes中，我们应该始终通过创建Controller来创建Pod，而不是直接创建Pod。控制器可以提供如下特性：

* 水平扩展（运行Pod的多个副本）
* 版本更新
* 故障恢复，如：当一个节点出现故障，控制器可以自动在另一个节点调度一个配置完全一样的Pod，以替换故障节点上的Pod


