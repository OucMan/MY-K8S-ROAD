# 1. 副本机制

在前面章节中，我们创建的Pod都是未经托管的，而是选择集群中的一个节点来运行Pod，然后在节点上运行容器，但是这样的话，当节点崩溃或者是网络断开，Pod不会自动被重新创建，对应的就是Pod丢失，另一个问题就是如果直接管理散乱分布在集群中的Pod十分复杂，其中包括如何保证集群内可用Pod的数量，如何为所有Pod更新镜像，更新镜像过程中，如何保证服务的可用性，如果实现快速回滚等等。

为了解决上面提到的问题，在K8s中，通常不会直接创建Pod，而是针对不同的场景通过创建不同的副本控制器来部署和发布Pod，本章节就对常见的副本控制器进行阐述。


# 2. ReplicationController

ReplicationController（简称rc）是一种K8s资源，可确保它的Pod始终保持运行状态。如果Pod因任何原因消失（如节点从集群中消失或由于该Pod已从节点中逐出），则rc会注意到缺少了Pod并创建替代Pod。

rc（以及下面所有的控制器）都是通过标签选择器来选择哪些Pod是归自己的管的，反过来，Pod的OwnerReference元数据会指向创建它的控制器。rc（以及下面所有的控制器）通过比较Spec中指定的Pod数量与实际运行的Pod数量，来决策是否创建Pod、删除Pod或者维持不变（控制循环的实现）。

## 2.1 创建ReplicationController

kubia-rc.yaml文件如下

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia:v1
        ports:
        - containerPort: 8080
```
文件解析

* apiVersion用来指明该描述文件遵循v1版本的K8s Api
* kind用来指明本段代码描述的是一个ReplicationController类型的资源
* metadata为该ReplicationController的元数据，其中设置了该ReplicationController的name为kubia
* spec用来阐述该ReplicationController的期望状态，里面有三个主要的部分，一个是replicas（副本个数）用来指定应运行的Pod数量，一个是selector（标签选择器），用来表明该ReplicationController根据app: kubia这一个标签来确定该Controller作用域中有哪些pod，一个是template（Pod模板），用于创建新的Pod副本，在本例中，该容器使用的镜像为luksa/kubia:v1，名字是kubia，同时暴露出的端口是tcp:8080。

*注：不指定标签选择器也是一种选择，在这种情况下，Controller会自动根据Pod模板中的标签自动配置*

接下来运行kubectl命令

```
sudo kubectl create -f kubia-rc.yaml
```

查看ReplicationController的创建情况
```
master@k8s-master:~$ sudo kubectl get rc
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       26s
```

查看ReplicationController的详细信息
```
master@k8s-master:~$ sudo kubectl describe rc kubia
Name:         kubia
Namespace:    default
Selector:     app=kubia
Labels:       app=kubia
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        luksa/kubia:v1
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  62s   replication-controller  Created pod: kubia-lsslw
  Normal  SuccessfulCreate  62s   replication-controller  Created pod: kubia-s2r6h
  Normal  SuccessfulCreate  62s   replication-controller  Created pod: kubia-pswhv
```

## 2.2 使用ReplicationController

首先查看一下通过ReplicationController创建的Pod情况
```
master@k8s-master:~$ sudo kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
kubia-lsslw   1/1     Running   0          13m
kubia-pswhv   1/1     Running   0          13m
kubia-s2r6h   1/1     Running   0          13m
```
我们查看利用如下命令来查看任一个pod的信息
```
sudo kubectl get pod kubia-lsslw -o yaml
```
可以看到Pod的ownerReferences元数据显示如下：
```
ownerReferences:
  - apiVersion: v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicationController
    name: kubia
    uid: 3f2b9712-3c70-4e2a-85ff-fcbe798a56a5
```
可见，该Pod确实是由kubia这个ReplicationController创建的。

下面尝试将Pod移入移除ReplicationController作用域。ReplicationController通过标签选择器来匹配Pod，因此可以通过修改Pod的标签来将其移入或者移除ReplicationController的作用域。

首先，我们修改其中一个Pod的app标签，然后再来查看一些会发生什么事情。

```
master@k8s-master:~$ sudo kubectl label pod kubia-lsslw app=nginx --overwrite
pod/kubia-lsslw labeled
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-lr57g   1/1     Running   0          6s    app=kubia
kubia-lsslw   1/1     Running   0          33m   app=nginx
kubia-pswhv   1/1     Running   0          33m   app=kubia
kubia-s2r6h   1/1     Running   0          33m   app=kubia
```
因为我们修改了kubia-lsslw这个Pod的标签（app=nginx），导致受ReplicationController管控的Pod数量减少到2，小于ReplicationController中指定的replicas（3），因此ReplicationController会重新创建一个Pod（kubia-lr57g）

再次测试，我们删除受ReplicationController管控的任一Pod，如kubia-s2r6h，来看看发生什么事情

```
master@k8s-master:~$ sudo kubectl delete pod kubia-s2r6h
pod "kubia-s2r6h" deleted
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE     LABELS
kubia-lr57g   1/1     Running   0          6m12s   app=kubia
kubia-lsslw   1/1     Running   0          39m     app=nginx
kubia-pswhv   1/1     Running   0          39m     app=kubia
kubia-sljhj   1/1     Running   0          94s     app=kubia
```

可以看到虽然Pod kubia-s2r6h被删除，但是ReplicationController又重新拉起了一个Pod kubia-sljhj，来保证当前集群中对应的可用Pod数量为3。

接下来我们再尝试将Pod kubia-lsslw的app标签改回来，这时候讲道理集群中会出现4个具有相同标签（app=kubia）的Pod，我们来看看ReplicationController会怎么做

 ```
master@k8s-master:~$ sudo kubectl label pod kubia-lsslw app=kubia --overwrite
pod/kubia-lsslw labeled
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS        RESTARTS   AGE     LABELS
kubia-lr57g   1/1     Running       0          10m     app=kubia
kubia-lsslw   1/1     Running       0          44m     app=kubia
kubia-pswhv   1/1     Running       0          44m     app=kubia
kubia-sljhj   1/1     Terminating   0          5m56s   app=kubia
 ```
 
可以看到ReplicationController终结了一个Pod kubia-sljhj，来保证当前集群中对应的可用Pod数量为3。
 
综合上面的实践，我们可以看到使用ReplicationController（其实还有其它控制器）来创建Pod，每时每刻都能够保证集群中可用Pod的数量。
 

## 2.3 水平缩放ReplicationController

使用kubectl scale rc rc_name --replicas=n命令实现ReplicationController的缩放

比如，我们将ReplicationController kubia的副本数量由3扩展到4

```
master@k8s-master:~$ sudo kubectl scale rc kubia --replicas=4
[sudo] password for master: 
replicationcontroller/kubia scaled
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-4kzwj   1/1     Running   0          5s    app=kubia
kubia-lr57g   1/1     Running   0          31m   app=kubia
kubia-lsslw   1/1     Running   0          64m   app=kubia
kubia-pswhv   1/1     Running   0          64m   app=kubia
```

然后再将ReplicationController kubia的副本数量由4扩展到3

```
master@k8s-master:~$ sudo kubectl scale rc kubia --replicas=3
replicationcontroller/kubia scaled
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS        RESTARTS   AGE   LABELS
kubia-4kzwj   1/1     Terminating   0          48s   app=kubia
kubia-lr57g   1/1     Running       0          31m   app=kubia
kubia-lsslw   1/1     Running       0          65m   app=kubia
kubia-pswhv   1/1     Running       0          65m   app=kubia
```

除了kubectl scale命令，还可以通过编辑定义来缩放ReplicationController，即通过命令
```
sudo kubectl edit rc kubia
```
打开ReplicationController kubia的定义文件，找到spec.replicas字段，将其更改为4，保存退出，接着查询生效

```
master@k8s-master:~$ sudo kubectl edit rc kubia
replicationcontroller/kubia edited
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-j8vj7   1/1     Running   0          5s    app=kubia
kubia-lr57g   1/1     Running   0          35m   app=kubia
kubia-lsslw   1/1     Running   0          69m   app=kubia
kubia-pswhv   1/1     Running   0          69m   app=kubia
```

## 2.4 移除ReplicationController

我们可以通过kubectl delete来删除ReplicationController，并且通过--cascade选项来决定，删除ReplicationController是否删除其管理的Pod

删除ReplicationController的同时也删除Pod
```
sudo kubectl delete rc kubia
```

只删除ReplicationController，保留Pod，但是此时Pod已经独立，不受任何控制器的管理。我们可以使用适当的标签选择器创建新的控制器将这些独立的Pod再管理起来。
```
sudo kubectl delete rc kubia --cascade=false
```

# 3. ReplicaSet

最初，ReplicationController是用于复制和异常时重新调度节点的唯一K8s组件，后面又引入了名为ReplicaSet这一个副本控制器将ReplicationController代替，但是两者的行为完全相同，只是ReplicaSet的标签选择器的表达能力更强。具体的，ReplicationController只能选择包含某一个标签匹配的Pod，而ReplicaSet还可以允许多个标签的同时匹配，还支持集合型选择器。换句话说ReplicaSet是表达能力更强的增强型ReplicationController，在其它方面，两种没有什么不同。

## 3.1 创建ReplicaSet

kubia-replicaset.yaml文件如下

```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia:v1
        ports:
        - containerPort: 8080
```
文件解析

* apiVersion用来指明该描述文件遵循apps/v1版本的K8s Api
* kind用来指明本段代码描述的是一个ReplicaSet类型的资源
* metadata为该ReplicaSet的元数据，其中设置了该ReplicaSet的name为kubia
* spec用来阐述该ReplicaSet的期望状态，里面有三个主要的部分，一个是replicas（副本个数）用来指定应运行的Pod数量，一个是selector（标签选择器），用来表明该ReplicaSet根据app: kubia这一个标签来确定该Controller作用域中有哪些pod，一个是template（Pod模板），用于创建新的Pod副本，在本例中，该容器使用的镜像为luksa/kubia:v1，名字是kubia，同时暴露出的端口是tcp:8080。


接下来运行kubectl命令

```
sudo kubectl create -f kubia-replicaset.yaml
```

查看ReplicaSet的创建情况
```
master@k8s-master:~$ sudo kubectl get rs
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       58s
```

查看ReplicaSet的详细信息
```
master@k8s-master:~$ sudo kubectl describe rs kubia
Name:         kubia
Namespace:    default
Selector:     app=kubia
Labels:       <none>
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        luksa/kubia:v1
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  77s   replicaset-controller  Created pod: kubia-stkxj
  Normal  SuccessfulCreate  77s   replicaset-controller  Created pod: kubia-fpz6j
  Normal  SuccessfulCreate  77s   replicaset-controller  Created pod: kubia-v4qkc
```

## 3.2 ReplicaSet中的标签选择器

ReplicaSet中的标签选择器具有比ReplicationController中的标签选择器更强大的表达能力，除了支持相等型选择器，还支持集合型选择器，并且支持多个选择器的同时满足。

### 3.2.1 相等型标签选择器模板

```
selector:
    matchLabels:
      app: kubia
      env: proc
```

matchLabels指定的是相等型选择器，上述选择器选择的是同时包含app=kubia和env=proc两个标签的Pod

### 3.2.2 集合型标签选择器模板

```
selector:
    matchExpressions:
      key: app
      operator: In
      values:
        - kubia
```

matchExpressions指定的是集合型选择器，每一个表达式都必须包含一个key和一个operator（运算符），可能还会包含一个values列表（取决于运算符）。

目前有四种有效的运算符：
* In: Lable的值必须与其中一个指定的values匹配
* NotIn: Lable的值与任意一个指定的values都不匹配
* Exists: Pod必须包含一个指定名称的标签（值不重要），在使用此运算符时，不应指定values字段
* DoesNotExist: Pod不得包含指定名称的标签，在使用此运算符时，不应指定values字段


### 3.2.3 多条件选择器

```
selector:
    matchLabels:
      app: kubia
      env: proc
    matchExpressions:
      key: app
      operator: In
      values:
        - kubia
```

当指定多个选择器时，各个选择器之间是逻辑与的关系，即所有选择器的条件都满足的Pod才会被选择。

### 3.3 其他操作

ReplicaSet与ReplicationController行为完全相同，因此其余操作参看ReplicationController即可，这里不再赘述。

# 4. Deployment

虽然上面描述了ReplicaSet与ReplicationController，但是一般情况下，管理员不会直接创建ReplicaSet与ReplicationController，而是通过创建Deployment这一个控制器来实现对Pod的创建和管理。本质上，创建Deployment后，Deployment控制器将立刻创建一个ReplicaSet副本集，并由ReplicaSet创建所需要的Pod，其目的就是在保留原来优点的前提下，更好地实现版本的更新与回滚。

## 4.1 创建Deployment

kubia-deployment.yaml文件如下

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia:v1
        ports:
        - containerPort: 8080
```
文件解析

* apiVersion用来指明该描述文件遵循apps/v1版本的K8s Api
* kind用来指明本段代码描述的是一个Deployment类型的资源
* metadata为该Deployment的元数据，其中设置了该Deployment的name为kubia
* spec用来阐述该Deployment的期望状态，里面有三个主要的部分，一个是replicas（副本个数）用来指定应运行的Pod数量，一个是selector（标签选择器），用来表明该ReplicaSet根据app: kubia这一个标签来确定该Controller作用域中有哪些pod，一个是template（Pod模板），用于创建新的Pod副本，在本例中，该容器使用的镜像为luksa/kubia:v1，名字是kubia，同时暴露出的端口是tcp:8080。

接下来运行kubectl命令

```
sudo kubectl create -f kubia-deployment.yaml
```

查看Deployment的创建情况
```
master@k8s-master:~$ sudo kubectl get deployment
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
kubia   3/3     3            3           13s

```

查看Deployment的详细信息
```
master@k8s-master:~$ sudo kubectl describe deployment kubia
Name:                   kubia
Namespace:              default
CreationTimestamp:      Tue, 22 Dec 2020 21:45:24 -0800
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=kubia
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        luksa/kubia:v1
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kubia-5f78bc5dc5 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  33s   deployment-controller  Scaled up replica set kubia-5f78bc5dc5 to 3
```

可以看到该Deployment创建了一个ReplicaSet，名称为kubia-5f78bc5dc5。

## 4.2 查看Pod状态

Deployment通过创建ReplicaSet进而创建和管理Pod。我们首先查看一下ReplicaSet kubia-5f78bc5dc5的ownerReferences元数据

```
sudo kubectl get rs kubia-5f78bc5dc5 -o yaml
```
可以看到
```
ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kubia
      uid: e8786a6a-ffb9-4e4c-bcc5-21eea0f2f8f2
```
说明ReplicaSet kubia-5f78bc5dc5确实是由Deployment kubia创建的，然后我们查看ReplicaSet kubia-5f78bc5dc5创建Pod的情况
```
master@k8s-master:~$ sudo kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
kubia-5f78bc5dc5-lcd2z   1/1     Running   0          8m45s
kubia-5f78bc5dc5-r4cwd   1/1     Running   0          8m45s
kubia-5f78bc5dc5-s65f7   1/1     Running   0          8m45s
```
通过查看任一Pod的ownerReferences元数据，确认这些Pod是由ReplicaSet kubia-5f78bc5dc5创建得到。

综上，证明了Deployment通过创建ReplicaSet进而创建和管理Pod。


## 4.3 更新镜像

我们来模拟更新Deployment镜像的过程，首先在worker节点上使用docker tag命令来创建一个新版本的镜像，如下
```
sudo docker tag luksa/kubia:v1 luksa/kubia:v2
```

首先查看当前的Deployment kubia的镜像，如下所示，采用的是luksa/kubia:v1
```
master@k8s-master:~$ sudo kubectl describe deployment kubia
Name:                   kubia
Namespace:              default
CreationTimestamp:      Tue, 22 Dec 2020 21:45:24 -0800
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=kubia
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        luksa/kubia:v1
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kubia-5f78bc5dc5 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  21m   deployment-controller  Scaled up replica set kubia-5f78bc5dc5 to 3
```

然后执行下面的命令将Deployment的镜像更改为luksa/kubia:v2,

```
sudo kubectl set image deployment.v1.apps/kubia kubia=luksa/kubia:v2
```
其中，deployment.v1.apps/kubia 为是固定写法，deployment是资源名、v1是资源版本、apps是资源组，这里也可以简写为deployment或者deployment.apps，比如说写为deployment的时候，默认将使用apps组 v1版本，后面的kubia为Deployment的名称。

后面的kubia=luksa/kubia:v2，kubia为Pod中容器的名字，因为Pod中可能有多个容器，因此要指定容器的名字，luksa/kubia:v2为镜像名。

```
master@k8s-master:~$ sudo kubectl set image deployment.v1.apps/kubia kubia=luksa/kubia:v2
deployment.apps/kubia image updated
master@k8s-master:~$ sudo kubectl describe deployment kubia
Name:                   kubia
Namespace:              default
CreationTimestamp:      Tue, 22 Dec 2020 21:45:24 -0800
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=kubia
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        luksa/kubia:v2
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   kubia-55d48448fd (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  29m   deployment-controller  Scaled up replica set kubia-5f78bc5dc5 to 3
  Normal  ScalingReplicaSet  7s    deployment-controller  Scaled up replica set kubia-55d48448fd to 1
  Normal  ScalingReplicaSet  6s    deployment-controller  Scaled down replica set kubia-5f78bc5dc5 to 2
  Normal  ScalingReplicaSet  6s    deployment-controller  Scaled up replica set kubia-55d48448fd to 2
  Normal  ScalingReplicaSet  4s    deployment-controller  Scaled down replica set kubia-5f78bc5dc5 to 1
  Normal  ScalingReplicaSet  4s    deployment-controller  Scaled up replica set kubia-55d48448fd to 3
  Normal  ScalingReplicaSet  2s    deployment-controller  Scaled down replica set kubia-5f78bc5dc5 to 0
```
可以看到镜像已经更新为luksa/kubia:v2。

## 4.4 滚动升级

在上文中通过更改镜像触发了Deployment的更新，那么Deployment是如何更新的呢。其实，Deployment的每一次更新都会创建新的ReplicaSet，在更新过程中，逐渐删除旧ReplicaSet管理的Pod，同时逐渐创建新ReplicaSet管理的Pod，从而使得在整个更新的过程中应用都处于一个可用状态，这种更新策略叫做滚动升级策略，是Deployment默认的更新策略。通过下面的信息我们可以看到滚动升级的过程。

```
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  29m   deployment-controller  Scaled up replica set kubia-5f78bc5dc5 to 3
  Normal  ScalingReplicaSet  7s    deployment-controller  Scaled up replica set kubia-55d48448fd to 1
  Normal  ScalingReplicaSet  6s    deployment-controller  Scaled down replica set kubia-5f78bc5dc5 to 2
  Normal  ScalingReplicaSet  6s    deployment-controller  Scaled up replica set kubia-55d48448fd to 2
  Normal  ScalingReplicaSet  4s    deployment-controller  Scaled down replica set kubia-5f78bc5dc5 to 1
  Normal  ScalingReplicaSet  4s    deployment-controller  Scaled up replica set kubia-55d48448fd to 3
  Normal  ScalingReplicaSet  2s    deployment-controller  Scaled down replica set kubia-5f78bc5dc5 to 0
```

查看一下ReplicaSet的情况

```
master@k8s-master:~$ sudo kubectl get rs
NAME               DESIRED   CURRENT   READY   AGE
kubia-55d48448fd   3         3         3       11m
kubia-5f78bc5dc5   0         0         0       41m
```
可以看到有两个ReplicaSet，这两个ReplicaSet都是由Deployment创建，并且旧ReplicaSet对应的可用Pod为0，新ReplicaSet对应的可用Pod为3。

在滚动升级的过程中，实际上还有一个**滚动升级策略**，其在yaml中定义如下

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    ...
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    ...
```
主要的就是设定maxSurge和maxUnavailable两个参数。

* maxSurge为滚动更新过程中，可以超出期望副本数的最大值，该取值可以是一个绝对值（如：5），也可以是一个相对于期望副本数的百分比（如：10%）；如果填写百分比，则以期望副本数乘以该百分比后向上取整的方式计算对应的绝对值；例如：假设此值被设定为30%，当滚动更新开始时，新的副本集（ReplicaSet）可以立刻扩容，但是旧Pod和新Pod的总数不超过Deployment期待副本数（spec.repilcas）的130%。一旦旧Pod被终止后，新的副本集可以进一步扩容，但是整个滚动更新过程中，新旧Pod的总数不超过Deployment期待副本数（spec.repilcas）的130%。
* maxUnavailable为滚动更新过程中，不可用副本数的最大值。该取值可以是一个绝对值（例如：5），也可以是一个相对于期望副本数的百分比（例如：10%）；如果填写百分比，则以期望副本数乘以该百分比后向下取整的方式计算对应的绝对值；当最大超出副本数maxSurge为0时，此数值不能为0；默认值为25%；例如：假设此值被设定为30%，当滚动更新开始时，旧的副本集（ReplicaSet）可以缩容到期望副本数的70%；在新副本集扩容的过程中，一旦新的Pod已就绪，旧的副本集可以进一步缩容，整个滚动更新过程中，确保新旧就绪副本数之和不低于期望副本数的70%。

此外在滚动升级过程中，可以使用如下命令来观察整个升级过程。

```
sudo kubectl rollout status deployment kubia
```

在滚动升级过程中，还可以执行暂定和恢复操作

```
sudo kubectl rollout pause deployment kubia
sudo kubectl rollout resume deployment kubia
```

此外，默认情况下滚动升级有一个期限（由spec中的progressDeadlineSeconds指定，默认为10分钟），也就是说在10分钟之内不能完成滚动升级的话将被视为失败。


## 4.5 回滚版本

首先利用kubectl rollout history可以查看升级历史
```
master@k8s-master:~$ sudo kubectl rollout history deployment kubia
deployment.apps/kubia 
REVISION  CHANGE-CAUSE
1         <none>
2         <none>
```
可以看到Deployment kubia更新了两次，但是我们不知道更新的原因是什么（CHANGE-CAUSE为空），因此强烈建议在执行创建或者更新操作命令时加上--record选项，如下

```
master@k8s-master:~$ sudo kubectl set image deployment.v1.apps/kubia kubia=luksa/kubia:v2 --record
deployment.apps/kubia image updated
master@k8s-master:~$ sudo kubectl rollout history deployment kubia
deployment.apps/kubia 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment.v1.apps/kubia kubia=luksa/kubia:v2 --record=true
```
--record选项本质上就是为资源增加了一个Annotations元数据
```
Annotations:            deployment.kubernetes.io/revision: 2
                        kubernetes.io/change-cause: kubectl set image deployment.v1.apps/kubia kubia=luksa/kubia:v2 --record=true
```

我们可以通过如下命令实现Deployment回滚到上一个版本

```
master@k8s-master:~$ sudo kubectl rollout undo deployment kubia
deployment.apps/kubia rolled back
master@k8s-master:~$ sudo kubectl rollout history deployment kubia
deployment.apps/kubia 
REVISION  CHANGE-CAUSE
2         kubectl set image deployment.v1.apps/kubia kubia=luksa/kubia:v2 --record=true
3         <none>
```
可以看到第一个版本现在变成了最新的版本了（第三个版本）

还可以利用如下命令实现将eployment回滚到指定版本

```
master@k8s-master:~$ sudo kubectl rollout undo deployment kubia --to-revision=2
deployment.apps/kubia rolled back
master@k8s-master:~$ sudo kubectl rollout history deployment kubia
deployment.apps/kubia 
REVISION  CHANGE-CAUSE
3         <none>
4         kubectl set image deployment.v1.apps/kubia kubia=luksa/kubia:v2 --record=true
```

在Deployment spec中有一个revisionHistoryLimit字段用来保证了保留历史版本的replicaset的数量，默认为10，也就是说历史版本存放10个。

## 4.6 Deployment状态

Deploymen 的生命周期中，将会进入不同的状态，这些状态可能是：
* Progressing：正在执行滚动更新
* Complete
* Fail to progress

### 4.6.1 Progressing状态

当如下任何一个任务正在执行时，Kubernete 将 Deployment 的状态标记为 progressing：

* Deployment 创建了一个新的 ReplicaSet
* Deployment 正在scale up其最新的 ReplicaSet
* Deployment 正在scale down其旧的 ReplicaSet
* 新的Pod变为就绪（ready） 或 可用（available）

### 4.6.2 Complete状态

如 Deployment符合以下条件，Kubernetes将其状态标记为 complete：

* 该Deployment中的所有Pod副本都已经被更新到指定的最新版本
* 该Deployment中的所有Pod副本都处于可用（available）状态
* 该Deployment中没有旧的ReplicaSet正在运行

### 4.6.3 Failed状态

Deployment在更新其最新的ReplicaSet时，可能卡住而不能达到complete状态。如下原因都可能导致此现象发生：

* 集群资源不够
* 就绪检查（readiness probe）失败
* 镜像抓取失败
* 权限不够
* 资源限制
* 应用程序的配置错误导致启动失败


## 4.7 总结

Deployment是一种十分常用的控制器，它通过创建ReplicaSet来创建Pod，并通过管理ReplicaSet来实现版本的滚动更新和回滚。 


# 5. DaemonSet

当需要在集群中的每个节点上运行一组Pod时，K8s提供了DaemonSet控制器。不像ReplicationController或者ReplicaSet，它们创建的Pod经由K8s调度器随机分布在集群中，DaemonSet控制器创建的Pod完全绕过调度器被分配到任何一个节点（或者被nodeSelector选择的节点）。

DaemonSet没有期望副本数的概念，因为它的工作是确保一个Pod匹配它的选择器并在每个节点上运行。如果节点下线，DaemonSet不会在其他地方重新创建Pod；当一个新节点添加到集群中，DaemonSet会立刻部署一个新的Pod实例；如果节点上的Pod被误删，DaemonSet会在该节点上重现创建一个Pod。

DaemonSet创建的Pod一般执行系统级别的与基础结构相关的操作，比如在每个节点上运行日志收集器（如logstash，fluentd）和资源监控器（如Promethues）。

DaemonSet创建的Pod部署到集群中的所有节点上，除非指定这些Pod只在部分节点上运行，这是通过Pod模板中的nodeSelector属性指定的。

## 5.1 DaemonSet演示

假设有一个ssd-monitor的守护进程，它需要在包含固态驱动器（SSD）的所有节点上运行。

### 5.1.1 创建监控镜像

*Dockerfile*
```
FROM busybox
ENTRYPOINT while true; do echo 'SSD OK'; sleep 5; done
```
文件解析

该监控镜像十分简单，基础镜像是busybox，执行的监控指令是每5秒打印一句“SSD OK”

注：这不是真正的监控程序，我们只是使用这个镜像来演示DaemonSet

执行构建镜像的命令
```
sudo docker build -f ./Dockerfile . -t luksa/ssd-monitor:v1
```

### 5.1.2 创建DaemonSet

*ssd-monitor-daemonset.yaml*
```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: luksa/ssd-monitor:v1
```

文件解析

* apiVersion用来指明该描述文件遵循apps/v1版本的K8s Api
* kind用来指明本段代码描述的是一个DaemonSet类型的资源
* metadata为该DaemonSet的元数据，其中设置了该Deployment的name为ssd-monitor
* spec用来阐述该DaemonSet的期望状态，里面有两个主要的部分，一个是selector（标签选择器），用来表明该ReplicaSet根据app: ssd-monitor这一个标签来确定该Controller作用域中有哪些pod，一个是template（Pod模板），用于创建新的Pod副本，在本例中，该容器使用的镜像为luksa/ssd-monitor:v1，名字是main，同时在节点选择器处表明，该Pod只会在具有标签disk: ssd的节点上部署。

执行创建命令
```
sudo kubectl create -f ssd-monitor-daemonset.yaml
```

查看一下创建的DaemonSet
```
master@k8s-master:~$ sudo kubectl get ds
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   0         0         0       0            0           disk=ssd        2m13s
```
发现DaemonSet并没有创建Pod，原因就是我们还没有给节点打上disk=ssd标签，因此DaemonSet没有找到合适的节点来部署Pod


### 5.1.3 向节点添加标签

我们发现目前集群中共有三个节点，一个master节点，两个worker节点
```
master@k8s-master:~$ sudo kubectl get nodes
NAME         STATUS   ROLES                  AGE   VERSION
k8s-master   Ready    control-plane,master   8d    v1.20.0
k8s-node1    Ready    <none>                 8d    v1.20.0
k8s-node2    Ready    <none>                 8d    v1.20.0
```

首先给k8s-node1增加disk=ssd的标签
```
master@k8s-master:~$ sudo kubectl label node k8s-node1 disk=ssd
node/k8s-node1 labeled
```
再来查看一下DaemonSet的情况
```
master@k8s-master:~$ sudo kubectl get ds
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   1         1         1       1            1           disk=ssd        8m24s
```
可以发现已经有一个Pod了，我们来看一下这个Pod的情况
```
master@k8s-master:~$ sudo kubectl get pods -o wide
NAME                READY   STATUS    RESTARTS   AGE     IP            NODE        NOMINATED NODE   READINESS GATES
ssd-monitor-gcw7t   1/1     Running   0          2m11s   10.244.1.19   k8s-node1   <none>           <none>
```
果然部署在了k8s-node1节点，进一步看看Pod的执行情况
```
master@k8s-master:~$ sudo kubectl logs ssd-monitor-gcw7t
SSD OK
SSD OK
SSD OK
SSD OK
SSD OK
SSD OK
SSD OK
SSD OK
...
```
也没有问题，接着我们也将k8s-node2增加disk=ssd的标签，接着来查看DaemonSet的情况
```
master@k8s-master:~$ sudo kubectl label node k8s-node2 disk=ssd
node/k8s-node2 labeled
master@k8s-master:~$ sudo kubectl get ds
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   2         2         2       2            2           disk=ssd        12m
master@k8s-master:~$ sudo kubectl get pods -o wide
NAME                READY   STATUS    RESTARTS   AGE    IP            NODE        NOMINATED NODE   READINESS GATES
ssd-monitor-8zbls   1/1     Running   0          55s    10.244.2.17   k8s-node2   <none>           <none>
ssd-monitor-gcw7t   1/1     Running   0          6m4s   10.244.1.19   k8s-node1   <none>           <none>

```
当标签添加完毕后，DaemonSet马上就会在k8s-node2上创建一个Pod

然后测试当节点上的标签被删除或者标签值被更改时，DaemonSet的响应如何？我们删除k8s-node2的disk标签（更改也是一样的），查看DaemonSet的情况，以及Pod的情况
```
master@k8s-master:~$ sudo kubectl label node k8s-node2 disk-
node/k8s-node2 labeled
master@k8s-master:~$ sudo kubectl get ds
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ssd-monitor   1         1         1       1            1           disk=ssd        16m
master@k8s-master:~$ sudo kubectl get pods -o wide
NAME                READY   STATUS        RESTARTS   AGE     IP            NODE        NOMINATED NODE   READINESS GATES
ssd-monitor-8zbls   1/1     Terminating   0          4m4s    10.244.2.17   k8s-node2   <none>           <none>
ssd-monitor-gcw7t   1/1     Running       0          9m13s   10.244.1.19   k8s-node1   <none>           <none>
```
可以观察到当修改k8s-node2的disk标签后，DaemonSet会删除运行在其上的Pod。

### 5.1.4 删除DaemonSet

执行下面的命令删除DaemonSet以及其管理的Pod
```
master@k8s-master:~$ sudo kubectl delete ds ssd-monitor
daemonset.apps "ssd-monitor" deleted
master@k8s-master:~$ sudo kubectl get ds
No resources found in default namespace.
master@k8s-master:~$ sudo kubectl get pods -o wide
NAME                READY   STATUS        RESTARTS   AGE   IP            NODE        NOMINATED NODE   READINESS GATES
ssd-monitor-gcw7t   1/1     Terminating   0          13m   10.244.1.19   k8s-node1   <none>           <none>
master@k8s-master:~$ 
```
如果在kubectl命令中指定--cascade=false选项，DaemonSet容器组将不会被删除。紧接着，如果您创建一个新的DaemonSet，与之前删除的DaemonSet有相同的.spec.selector，新建DaemonSet将直接把这些未删除的Pod纳入管理。

## 5.2 DaemonSet更新

同Deployment一样，DaemonSet默认的更新策略是滚动更新，这里不再赘述，当然也可以手动删除旧镜像创建的Pod，从而DaemonSet使用新镜像重新拉起Pod。

# 6. Job

上述的控制器创建的Pod都是持续运行的，假如将要创建在运行完工作后就终止，面向临时任务的Pod，就需要使用到Job。

Job的工作机制如下：当节点发生故障时，该节点上由Job管理的Pod将按照其它控制器的Pod的方式重新被安排到其它节点；如果Pod中的容器（业务进程）异常退出（进程返回错误退出代码），Job配置为重新启动容器。

Kubernetes中的Job对象将创建一个或多Pod，并确保指定数量的Pod可以成功执行到进程正常结束：

* 当Job创建的Pod执行成功并正常结束时，Job将记录成功结束的Pod数量
* 当成功结束的Pod达到指定的数量时，Job将完成执行
* 删除Job对象时，将清理掉由Job创建的Pod

## 6.1 Job演示

假设有数据存储在某个地方，需要转换并将其导出到某个地方，我们使用sleep命令来模拟这个耗时操作。

### 6.1.1 创建镜像

*Dockerfile*
```
FROM busybox
ENTRYPOINT echo "$(date) Batch job starting"; sleep 120; echo "$(date) Finished succesfully"
```
基础镜像是busybox，执行三条命令：打印任务开始标识，睡眠2分钟模拟耗时操作，打印任务结束标识

执行构建镜像的命令
```
sudo docker build -f ./Dockerfile . -t luksa/batch-job:v1
```

### 6.1.2 Job运行一个Pod

*batch-job.yaml*
```
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job:v1
  backoffLimit: 4
```

文件解析

* apiVersion用来指明该描述文件遵循batch/v1版本的K8s Api
* kind用来指明本段代码描述的是一个Job类型的资源
* metadata为该Job的元数据，其中设置了该Job的name为batch-job
* spec用来阐述该Job的期望状态，里面有两个主要的部分，一个是template（Pod模板），用于创建新的Pod副本，在本例中，该容器使用的镜像为luksa/batch-job:v1，名字是main，同时指明容器的restartPolicy（重启策略）为OnFailure；第二个是backoffLimit（Job重试次数，即Pod失败后重新拉起的次数）

可以发现，Job的定义和其它控制器的定义最主要的区别就是restartPolicy容器重启策略和backoffLimit重试次数限制。

restartPolicy重启策略由三种：Never（不论容器运行状态如何，kubelet都不会重启该容器）、OnFailure（当容器终止运行且退出码不为0时，由kubelet自动重启该容器）、Always（当容器失效时，由kubelet自动重启该容器）；在Job定义中，不能使用默认策略Always，因为它们不是要无限期地运行，因此需要明确地将重启策略设置为OnFailure或者Never，此设置防止容器在完成任务时重新启动。

当Pod执行失败时，Job在运行的时候不可能去无限地重试，所以需要一个参数来控制重试的次数，这个backoffLimit就是来保证一个Job到底能重试多少次。

执行创建命令
```
sudo kubectl create -f batch-job.yaml
```
接着看一下Job的创建结果，会发现该Job会立即启动一个Pod
```
master@k8s-master:~$ sudo kubectl get jobs
NAME        COMPLETIONS   DURATION   AGE
batch-job   0/1           20s        20s
master@k8s-master:~$ sudo kubectl get pods
NAME              READY   STATUS    RESTARTS   AGE
batch-job-wsvst   1/1     Running   0          46s
```
我们还发现batch-job的COMPLETIONS为0/1，意思就是当前正在运行着一个Pod（即batch-job-wsvst，状态是Running），还没有完成的Pod。吃个苹果等两分钟，我们再看一下Job和Pod的情况
```
master@k8s-master:~$ sudo kubectl get jobs
NAME        COMPLETIONS   DURATION   AGE
batch-job   1/1           2m2s       4m44s
master@k8s-master:~$ sudo kubectl get pods
NAME              READY   STATUS      RESTARTS   AGE
batch-job-wsvst   0/1     Completed   0          4m48s
```
发现Pod执行完毕，状态变为Completed，batch-job的COMPLETIONS也变成了1/1。这里虽然Pod执行结束，但是没有被删除，其原因就是给我们留个机会去查阅它的日志，发现正是我们程序中打印出来的，爽！！！
```
master@k8s-master:~$ sudo kubectl logs batch-job-wsvst
Thu Dec 24 03:40:01 UTC 2020 Batch job starting
Thu Dec 24 03:42:01 UTC 2020 Finished succesfully
```
演示差不多了，删除该Job
```
master@k8s-master:~$ sudo kubectl delete job batch-job
job.batch "batch-job" deleted
master@k8s-master:~$ sudo kubectl get jobs
No resources found in default namespace.
master@k8s-master:~$ sudo kubectl get pods
No resources found in default namespace.
```

### 6.1.3 Job运行多个Pod实例（串行）

假如需要一个Job运行多次，那么可以通过设置spec.completions字段来调整希望作业运行的次数，因此更新yaml文件如下：

*multi-completion-batch-job.yaml*
```
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job:v1
  backoffLimit: 4
  completions: 5
```
我们将spec.completions设置为5，因此Job将一个接一个地运行5个Pod，当其中一个Pod发生故障，Job会创建一个新的Pod，因此Job总共创建至少5个Pod。

执行创建命令
```
sudo kubectl create -f multi-completion-batch-job.yaml
```

查看初始的状态
```
master@k8s-master:~$ sudo kubectl get jobs
NAME        COMPLETIONS   DURATION   AGE
batch-job   0/5           12s        12s
master@k8s-master:~$ sudo kubectl get pods
NAME              READY   STATUS    RESTARTS   AGE
batch-job-qqt5q   1/1     Running   0          21s
```

查看中间状态
```
master@k8s-master:~$ sudo kubectl get jobs
NAME        COMPLETIONS   DURATION   AGE
batch-job   2/5           5m6s       5m6s
master@k8s-master:~$ sudo kubectl get pods
NAME              READY   STATUS      RESTARTS   AGE
batch-job-cm2sj   1/1     Running     0          64s
batch-job-flt2c   0/1     Completed   0          3m6s
batch-job-qqt5q   0/1     Completed   0          5m8s
```

查看最终状态
```
master@k8s-master:~$ sudo kubectl get jobs
NAME        COMPLETIONS   DURATION   AGE
batch-job   5/5           10m        10m
master@k8s-master:~$ sudo kubectl get pods
NAME              READY   STATUS      RESTARTS   AGE
batch-job-bgmxz   0/1     Completed   0          2m12s
batch-job-cm2sj   0/1     Completed   0          6m16s
batch-job-flt2c   0/1     Completed   0          8m18s
batch-job-lbj9b   0/1     Completed   0          4m14s
batch-job-qqt5q   0/1     Completed   0          10m
```
Pod之间的时间间隔大致为2分钟

删除Job
```
master@k8s-master:~$ sudo kubectl delete job batch-job
job.batch "batch-job" deleted
master@k8s-master:~$ sudo kubectl get pods
No resources found in default namespace.
```

### 6.1.4 Job运行多个Pod实例（并行）

上面的演示是一个接一个地单个运行Pod，还可以让Job并行运行多个Pod，该设置是通过spec.parallelism字段来实现，该字段用来指定允许多少个Pod并行执行，因此更新yaml文件如下：

*multi-completion-parallel-batch-job.yaml*
```
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job:v1
  backoffLimit: 4
  completions: 5
  parallelism: 2
```
我们将spec.parallelism设置为2，因此Job将在一个批次创建两个Pod并行运行

执行创建命令
```
sudo kubectl create -f multi-completion-parallel-batch-job.yaml
```

查看初始的状态
```
master@k8s-master:~$ sudo kubectl get jobs
NAME        COMPLETIONS   DURATION   AGE
batch-job   0/5           15s        15s
master@k8s-master:~$ sudo kubectl get pods
NAME              READY   STATUS    RESTARTS   AGE
batch-job-59cs9   1/1     Running   0          19s
batch-job-llfwn   1/1     Running   0          19s
```

查看最终状态
```
master@k8s-master:~$ sudo kubectl get jobs
NAME        COMPLETIONS   DURATION   AGE
batch-job   5/5           6m6s       20m
master@k8s-master:~$ sudo kubectl get pods
NAME              READY   STATUS      RESTARTS   AGE
batch-job-2lqnl   0/1     Completed   0          18m
batch-job-4v7wx   0/1     Completed   0          18m
batch-job-52kjk   0/1     Completed   0          16m
batch-job-59cs9   0/1     Completed   0          20m
batch-job-llfwn   0/1     Completed   0          20m
```

根据时间可以看出，每一批次运行两个Pod

删除Job
```
master@k8s-master:~$ sudo kubectl delete job batch-job
job.batch "batch-job" deleted
master@k8s-master:~$ sudo kubectl get pods
No resources found in default namespace.
```

## 6.2 限制Job Pod完成任务的时间

假如Pod卡住了并且根本无法完成（或者无法完成得足够快），难道Job会一直等下去吗？肯定不行，因此Job通过在Pod配置中设置activeDeadlineSeconds属性来限制Pod的时间，如果Pod运行时间超过此时间，系统将尝试终止Pod，并重新创建Pod，假如重新创建的次数大于spec.backoffLimit，则将Job标记为失败。


# 7.CronJob


# 8. StatefulSet


# 9. 总结

Pod是Kubernetes中最小的调度单元，我们可以通过kubectl直接创建一个Pod，但是Pod本身并不能自愈（self-healing）。如果一个Pod所在的Node出现故障，Pod将被删除；同理，当因为节点资源不够或节点维护而驱逐Pod时，Pod也将被删除。

Kubernetes通过引入副本控制器的概念来管理Pod实例。在Kubernetes中，我们应该始终通过创建Controller来创建Pod，而不是直接创建Pod。控制器可以提供如下特性：

* 水平扩展（运行Pod的多个副本）
* 版本更新
* 故障恢复，如：当一个节点出现故障，控制器可以自动在另一个节点调度一个配置完全一样的Pod，以替换故障节点上的Pod


