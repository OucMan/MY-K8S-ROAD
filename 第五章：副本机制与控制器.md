# 1. 副本机制

在前面章节中，我们创建的Pod都是未经托管的，而是选择集群中的一个节点来运行Pod，然后在节点上运行容器，但是这样的话，当节点崩溃或者是网络断开，Pod不会自动被重新创建，对应的就是Pod丢失，另一个问题就是如果直接管理散乱分布在集群中的Pod十分复杂，其中包括如何保证集群内可用Pod的数量，如何为所有Pod更新镜像，更新镜像过程中，如何保证服务的可用性，如果实现快速回滚等等。

为了解决上面提到的问题，在K8s中，通常不会直接创建Pod，而是针对不同的场景通过创建不同的副本控制器来部署和发布Pod，本章节就对常见的副本控制器进行阐述。


# 2. ReplicationController

ReplicationController（简称rc）是一种K8s资源，可确保它的Pod始终保持运行状态。如果Pod因任何原因消失（如节点从集群中消失或由于该Pod已从节点中逐出），则rc会注意到缺少了Pod并创建替代Pod。

rc（以及下面所有的控制器）都是通过标签选择器来选择哪些Pod是归自己的管的，反过来，Pod的OwnerReference元数据会指向创建它的控制器。rc（以及下面所有的控制器）通过比较Spec中指定的Pod数量与实际运行的Pod数量，来决策是否创建Pod、删除Pod或者维持不变（控制循环的实现）。

## 2.1 创建ReplicationController

kubia-rc.yaml文件如下

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia:v1
        ports:
        - containerPort: 8080
```
文件解析

* apiVersion用来指明该描述文件遵循v1版本的K8s Api
* kind用来指明本段代码描述的是一个ReplicationController类型的资源
* metadata为该Pod的元数据，其中设置了该ReplicationController的name为kubia-manual
* spec用来阐述该ReplicationController的期望状态，里面有三个主要的部分，一个是replicas（副本个数）用来指定应运行的Pod数量，一个是selector（标签选择器），用来表明该ReplicationController根据app: kubia这一个标签来确定该Controller作用域中有哪些pod，一个是template（Pod模板），用于创建新的Pod副本，在本例中，该容器使用的镜像为luksa/kubia:v1，名字是kubia，同时暴露出的端口是tcp:8080。

*注：不指定标签选择器也是一种选择，在这种情况下，Controller会自动根据Pod模板中的标签自动配置*

接下来运行kubectl命令

```
sudo kubectl create -f kubia-rc.yaml
```

查看ReplicationController的创建情况
```
master@k8s-master:~$ sudo kubectl get rc
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       26s
```

查看ReplicationController的详细信息
```
master@k8s-master:~$ sudo kubectl describe rc kubia
Name:         kubia
Namespace:    default
Selector:     app=kubia
Labels:       app=kubia
Annotations:  <none>
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=kubia
  Containers:
   kubia:
    Image:        luksa/kubia:v1
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From                    Message
  ----    ------            ----  ----                    -------
  Normal  SuccessfulCreate  62s   replication-controller  Created pod: kubia-lsslw
  Normal  SuccessfulCreate  62s   replication-controller  Created pod: kubia-s2r6h
  Normal  SuccessfulCreate  62s   replication-controller  Created pod: kubia-pswhv
```

## 2.2 使用ReplicationController

首先查看一下通过ReplicationController创建的Pod情况
```
master@k8s-master:~$ sudo kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
kubia-lsslw   1/1     Running   0          13m
kubia-pswhv   1/1     Running   0          13m
kubia-s2r6h   1/1     Running   0          13m
```
我们查看利用如下命令来查看任一个pod的信息
```
sudo kubectl get pod kubia-lsslw -o yaml
```
可以看到Pod的ownerReferences元数据显示如下：
```
ownerReferences:
  - apiVersion: v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicationController
    name: kubia
    uid: 3f2b9712-3c70-4e2a-85ff-fcbe798a56a5
```
可见，该Pod确实是由kubia这个ReplicationController创建的。

下面尝试将Pod移入移除ReplicationController作用域。ReplicationController通过标签选择器来匹配Pod，因此可以通过修改Pod的标签来将其移入或者移除ReplicationController的作用域。

首先，我们修改其中一个Pod的app标签，然后再来查看一些会发生什么事情。

```
master@k8s-master:~$ sudo kubectl label pod kubia-lsslw app=nginx --overwrite
pod/kubia-lsslw labeled
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-lr57g   1/1     Running   0          6s    app=kubia
kubia-lsslw   1/1     Running   0          33m   app=nginx
kubia-pswhv   1/1     Running   0          33m   app=kubia
kubia-s2r6h   1/1     Running   0          33m   app=kubia
```
因为我们修改了kubia-lsslw这个Pod的标签（app=nginx），导致受ReplicationController管控的Pod数量减少到2，小于ReplicationController中指定的replicas（3），因此ReplicationController会重新创建一个Pod（kubia-lr57g）

再次测试，我们删除受ReplicationController管控的任一Pod，如kubia-s2r6h，来看看发生什么事情

```
master@k8s-master:~$ sudo kubectl delete pod kubia-s2r6h
pod "kubia-s2r6h" deleted
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE     LABELS
kubia-lr57g   1/1     Running   0          6m12s   app=kubia
kubia-lsslw   1/1     Running   0          39m     app=nginx
kubia-pswhv   1/1     Running   0          39m     app=kubia
kubia-sljhj   1/1     Running   0          94s     app=kubia
```

可以看到虽然Pod kubia-s2r6h被删除，但是ReplicationController又重新拉起了一个Pod kubia-sljhj，来保证当前集群中对应的可用Pod数量为3。

接下来我们再尝试将Pod kubia-lsslw的app标签改回来，这时候讲道理集群中会出现4个具有相同标签（app=kubia）的Pod，我们来看看ReplicationController会怎么做

 ```
master@k8s-master:~$ sudo kubectl label pod kubia-lsslw app=kubia --overwrite
pod/kubia-lsslw labeled
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS        RESTARTS   AGE     LABELS
kubia-lr57g   1/1     Running       0          10m     app=kubia
kubia-lsslw   1/1     Running       0          44m     app=kubia
kubia-pswhv   1/1     Running       0          44m     app=kubia
kubia-sljhj   1/1     Terminating   0          5m56s   app=kubia
 ```
 
可以看到ReplicationController终结了一个Pod kubia-sljhj，来保证当前集群中对应的可用Pod数量为3。
 
综合上面的实践，我们可以看到使用ReplicationController（其实还有其它控制器）来创建Pod，每时每刻都能够保证集群中可用Pod的数量。
 

## 2.3 水平缩放ReplicationController

使用kubectl scale rc rc_name --replicas=n命令实现ReplicationController的缩放

比如，我们将ReplicationController kubia的副本数量由3扩展到4

```
master@k8s-master:~$ sudo kubectl scale rc kubia --replicas=4
[sudo] password for master: 
replicationcontroller/kubia scaled
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-4kzwj   1/1     Running   0          5s    app=kubia
kubia-lr57g   1/1     Running   0          31m   app=kubia
kubia-lsslw   1/1     Running   0          64m   app=kubia
kubia-pswhv   1/1     Running   0          64m   app=kubia
```

然后再将ReplicationController kubia的副本数量由4扩展到3

```
master@k8s-master:~$ sudo kubectl scale rc kubia --replicas=3
replicationcontroller/kubia scaled
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS        RESTARTS   AGE   LABELS
kubia-4kzwj   1/1     Terminating   0          48s   app=kubia
kubia-lr57g   1/1     Running       0          31m   app=kubia
kubia-lsslw   1/1     Running       0          65m   app=kubia
kubia-pswhv   1/1     Running       0          65m   app=kubia
```

除了kubectl scale命令，还可以通过编辑定义来缩放ReplicationController，即通过命令
```
sudo kubectl edit rc kubia
```
打开ReplicationController kubia的定义文件，找到spec.replicas字段，将其更改为4，保存退出，接着查询生效

```
master@k8s-master:~$ sudo kubectl edit rc kubia
replicationcontroller/kubia edited
master@k8s-master:~$ sudo kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-j8vj7   1/1     Running   0          5s    app=kubia
kubia-lr57g   1/1     Running   0          35m   app=kubia
kubia-lsslw   1/1     Running   0          69m   app=kubia
kubia-pswhv   1/1     Running   0          69m   app=kubia
```

## 2.4 移除ReplicationController

我们可以通过kubectl delete来删除ReplicationController，并且通过--cascade选项来决定，删除ReplicationController是否删除其管理的Pod

删除ReplicationController的同时也删除Pod
```
sudo kubectl delete rc kubia
```

只删除ReplicationController，保留Pod，但是此时Pod已经独立，不受任何控制器的管理。我们可以使用适当的标签选择器创建新的控制器将这些独立的Pod再管理起来。
```
sudo kubectl delete rc kubia --cascade=false
```

# 3. ReplicaSet




# 4. Deployment


# 5. DaemonSet


# 6. Job


# 7.CronJob


# 8. StatefulSet


# 9. 总结

Pod是Kubernetes中最小的调度单元，我们可以通过kubectl直接创建一个Pod，但是Pod本身并不能自愈（self-healing）。如果一个Pod所在的Node出现故障，Pod将被删除；同理，当因为节点资源不够或节点维护而驱逐Pod时，Pod也将被删除。

Kubernetes通过引入副本控制器的概念来管理Pod实例。在Kubernetes中，我们应该始终通过创建Controller来创建Pod，而不是直接创建Pod。控制器可以提供如下特性：

* 水平扩展（运行Pod的多个副本）
* 版本更新
* 故障恢复，如：当一个节点出现故障，控制器可以自动在另一个节点调度一个配置完全一样的Pod，以替换故障节点上的Pod


